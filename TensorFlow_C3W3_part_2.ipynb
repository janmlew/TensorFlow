{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO5rlD6DjCIojTN8kF8CyvI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janmlew/TensorFlow/blob/master/TensorFlow_C3W3_part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ungraded Lab: Multiple LSTMs\n",
        "In this lab, you will look at how to build a model with multiple LSTM layers. Since you know the preceding steps already (e.g. downloading datasets, preparing the data, etc.), we won't expound on it anymore so you can just focus on the model building code.\n",
        "\n",
        "# Download and Prepare the Dataset"
      ],
      "metadata": {
        "id": "WG5lAlnMGO9w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMYFSoHzGNBR"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Download the subword encoded pretokenized dataset\n",
        "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)\n",
        "\n",
        "# Get the tokenizer\n",
        "tokenizer = info.features['text'].encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like the previous lab, we increased the BATCH_SIZE here to make the training faster. If you are doing this on your local machine and have a powerful processor, feel free to use the value used in the lecture (i.e. 64) to get the same results as Laurence."
      ],
      "metadata": {
        "id": "PWAJJPDDgZmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# Get the train and test splits\n",
        "train_data, test_data = dataset['train'], dataset['test'],\n",
        "\n",
        "# Shuffle the training data\n",
        "train_dataset = train_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "# Batch and pad the datasets to the maximum length of the sequences\n",
        "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
        "test_dataset = test_data.padded_batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "6Nar9MLOgd5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and Compile the Model\n",
        "You can build multiple layer LSTM models by simply appending another LSTM layer in your Sequential model and enabling the return_sequences flag to True. This is because an LSTM layer expects a sequence input so if the previous layer is also an LSTM, then it should output a sequence as well. See the code cell below that demonstrates this flag in action. You'll notice that the output dimension is in 3 dimensions (batch_size, timesteps, features) when when return_sequences is True."
      ],
      "metadata": {
        "id": "pXCkwQd-gcBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 1\n",
        "timesteps = 20\n",
        "features = 16\n",
        "lstm_dim = 8\n",
        "\n",
        "print(f'batch_size: {batch_size}')\n",
        "print(f'timesteps (sequence length): {timesteps}')\n",
        "print(f'features (embedding size): {features}')\n",
        "print(f'lstm output units: {lstm_dim}')\n",
        "\n",
        "# Define array input with random values\n",
        "random_input = np.random.rand(batch_size,timesteps,features)\n",
        "print(f'shape of input array: {random_input.shape}')\n",
        "\n",
        "# Define LSTM that returns a single output\n",
        "lstm = tf.keras.layers.LSTM(lstm_dim)\n",
        "result = lstm(random_input)\n",
        "print(f'shape of lstm output(return_sequences=False): {result.shape}')\n",
        "\n",
        "# Define LSTM that returns a sequence\n",
        "lstm_rs = tf.keras.layers.LSTM(lstm_dim, return_sequences=True)\n",
        "result = lstm_rs(random_input)\n",
        "print(f'shape of lstm output(return_sequences=True): {result.shape}')"
      ],
      "metadata": {
        "id": "Uu1eNhLJgily"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell implements the stacked LSTM architecture."
      ],
      "metadata": {
        "id": "SxizgIRBglrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 64\n",
        "lstm1_dim = 64\n",
        "lstm2_dim = 32\n",
        "dense_dim = 64\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(tokenizer.vocab_size, embedding_dim),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm1_dim, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm2_dim)),\n",
        "    tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "CqCPS8LOgnWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the training parameters\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "H3MEfMyBgqYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model\n",
        "The additional LSTM layer will lengthen the training time compared to the previous lab. Given the default parameters we set, it will take around 2 minutes per epoch with the Colab GPU enabled."
      ],
      "metadata": {
        "id": "1krhR49VgpCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_dataset, epochs=NUM_EPOCHS, validation_data=test_dataset)"
      ],
      "metadata": {
        "id": "D-15VpkpguKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot utility\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "\n",
        "# Plot the accuracy and results\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "metadata": {
        "id": "p_Wa3p7KgvqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrap Up\n",
        "This lab showed how you can build deep networks by stacking LSTM layers. In the next labs, you will continue exploring other architectures you can use to implement your sentiment classification model."
      ],
      "metadata": {
        "id": "9Kcv8b9HgxlD"
      }
    }
  ]
}